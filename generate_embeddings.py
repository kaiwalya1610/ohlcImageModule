"""
Unified Embedding Generator for Images and Time-Series

This script scans the dataset generated by generate_dinov3_dataset.py,
generates Qwen3-VL embeddings for images and time-series (as text strings),
and saves them to a unified .pt file for later retrieval.
"""

import os
from pathlib import Path

import numpy as np
import pandas as pd
import torch
from src.models.qwen3_vl_embedding import Qwen3VLEmbedder, is_image_path


# ============================================================================
# SETTINGS - Edit these variables to match your setup
# ============================================================================

# Dataset root directory (output from generate_dinov3_dataset.py)
DATASET_ROOT = Path("dinov3_nifty50_dataset")

# Metadata CSV path
METADATA_CSV = DATASET_ROOT / "metadata" / "dataset-TRAIN.csv"

# Where to save the unified embeddings (.pt file)
OUTPUT_PATH = "./embeddings_unified.pt"

# Model to use for embedding generation
MODEL_PATH = "Qwen/Qwen3-VL-Embedding-8B"

# Batch size for processing (lower = less VRAM, slower)
BATCH_SIZE = 4


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def load_dataset_paths(metadata_csv: Path, dataset_root: Path):
    """Load image and vector paths from metadata CSV.

    Args:
        metadata_csv: Path to the metadata CSV file
        dataset_root: Root directory of the dataset

    Returns:
        Tuple of (image_paths, vector_paths, metadata_df)
    """
    if not metadata_csv.exists():
        raise FileNotFoundError(f"Metadata CSV not found: {metadata_csv}")

    df = pd.read_csv(metadata_csv)
    print(f"Loaded metadata: {len(df)} entries")

    # Build absolute image paths
    image_paths = []
    for _, row in df.iterrows():
        img_path = dataset_root / "images" / row['image_name']
        image_paths.append(str(img_path.absolute()))

    # Build absolute vector paths (filter successful vectors only)
    vector_paths = []
    for _, row in df.iterrows():
        if row.get('vector_status') == 'success' and pd.notna(row.get('vector_path')):
            vec_path = dataset_root / row['vector_path']
            vector_paths.append(str(vec_path.absolute()))

    print(f"Found {len(image_paths)} images, {len(vector_paths)} vectors")
    return image_paths, vector_paths, df


def load_ts_vectors_as_text(vector_paths: list) -> list:
    """Load time-series vectors from .npy files and convert to text strings.

    Args:
        vector_paths: List of paths to .npy vector files

    Returns:
        List of text strings (numpy array as string)
    """
    if not vector_paths:
        return []

    texts = []
    for path in vector_paths:
        vec = np.load(path)
        # Convert numpy array directly to string
        text = str(vec)
        texts.append(text)

    print(f"Loaded {len(texts)} vectors as text")
    return texts


def find_images(folder_path: str) -> list:
    """Recursively find all valid image files in a folder.

    Returns a list of absolute paths.
    """
    image_paths = []
    for root, _, files in os.walk(folder_path):
        for file in files:
            full_path = os.path.join(root, file)
            if is_image_path(full_path):
                image_paths.append(os.path.abspath(full_path))
    return sorted(image_paths)


# ============================================================================
# MAIN LOGIC
# ============================================================================

def main():
    print("=" * 70)
    print("Unified Embedding Generator (Images + Time-Series)")
    print("=" * 70)

    # Step 1: Load dataset paths from metadata
    print(f"\nStep 1: Loading dataset from {DATASET_ROOT}")
    image_paths, vector_paths, metadata_df = load_dataset_paths(
        METADATA_CSV, DATASET_ROOT
    )

    if not image_paths:
        print("No images found! Check DATASET_ROOT and METADATA_CSV paths.")
        return

    # Step 2: Load the embedding model
    print(f"\nStep 2: Loading Qwen3-VL model: {MODEL_PATH}")
    model = Qwen3VLEmbedder(model_name_or_path=MODEL_PATH)
    print("Model loaded!")

    # Step 3: Generate image embeddings in batches
    print(f"\nStep 3: Generating image embeddings ({len(image_paths)} images)")
    all_image_embeddings = []
    total_batches = (len(image_paths) + BATCH_SIZE - 1) // BATCH_SIZE

    for batch_idx in range(total_batches):
        start_idx = batch_idx * BATCH_SIZE
        end_idx = min(start_idx + BATCH_SIZE, len(image_paths))
        batch_paths = image_paths[start_idx:end_idx]

        print(f"  Batch {batch_idx + 1}/{total_batches} ({len(batch_paths)} images)")

        # Format inputs for the model
        inputs = [{"image": path} for path in batch_paths]

        # Generate embeddings
        embeddings = model.process(inputs)
        all_image_embeddings.append(embeddings.cpu())

    # Concatenate all batch embeddings
    image_embeddings = torch.cat(all_image_embeddings, dim=0)
    print(f"  Image embeddings shape: {image_embeddings.shape}")

    # Step 4: Load time-series vectors as text
    print(f"\nStep 4: Loading time-series vectors as text ({len(vector_paths)} vectors)")
    ts_texts = load_ts_vectors_as_text(vector_paths)

    ts_embeddings = None

    if len(ts_texts) > 0:
        # Step 5: Generate text embeddings for time-series
        print(f"\nStep 5: Generating time-series embeddings ({len(ts_texts)} texts)")
        all_ts_embeddings = []
        total_ts_batches = (len(ts_texts) + BATCH_SIZE - 1) // BATCH_SIZE

        for batch_idx in range(total_ts_batches):
            start_idx = batch_idx * BATCH_SIZE
            end_idx = min(start_idx + BATCH_SIZE, len(ts_texts))
            batch_texts = ts_texts[start_idx:end_idx]

            print(f"  Batch {batch_idx + 1}/{total_ts_batches} ({len(batch_texts)} texts)")

            # Format inputs for the model (text)
            inputs = [{"text": text} for text in batch_texts]

            # Generate embeddings
            embeddings = model.process(inputs)
            all_ts_embeddings.append(embeddings.cpu())

        # Concatenate all batch embeddings
        ts_embeddings = torch.cat(all_ts_embeddings, dim=0)
        print(f"  Time-series embeddings shape: {ts_embeddings.shape}")
    else:
        print("  No time-series vectors found")
        ts_embeddings = torch.tensor([])

    # Free model memory
    del model
    torch.cuda.empty_cache()

    # Step 6: Save unified output
    print(f"\nStep 6: Saving unified embeddings to {OUTPUT_PATH}")
    output_data = {
        "image_embeddings": image_embeddings,
        "image_paths": image_paths,
        "ts_embeddings": ts_embeddings,
        "ts_paths": vector_paths,
        "ts_texts": ts_texts,
    }

    torch.save(output_data, OUTPUT_PATH)

    print("\n" + "=" * 70)
    print("Embedding generation complete!")
    print("=" * 70)
    print(f"  Images: {len(image_paths)} ({image_embeddings.shape})")
    if ts_embeddings is not None and ts_embeddings.numel() > 0:
        print(f"  Time-series: {len(vector_paths)} ({ts_embeddings.shape})")
    else:
        print(f"  Time-series: None")
    print(f"  Output: {OUTPUT_PATH}")


if __name__ == "__main__":
    main()
