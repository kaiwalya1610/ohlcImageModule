#!/usr/bin/env python3
"""
Time-Series Embedding Generator using Qwen3-Embedding-4B

This script generates embeddings for time-series vectors (not images) from the
DINOv3 dataset. It converts .npy vectors to text strings and uses Qwen3-Embedding-4B
to generate embeddings for retrieval and similarity search.

Approach:
- Loads .npy time-series vectors generated by generate_dinov3_dataset.py
- Converts vectors to text strings (str(array))
- Generates embeddings using Qwen3-Embedding-4B from Hugging Face
- Saves embeddings for efficient retrieval
- Creates FAISS index for similarity search
"""
from __future__ import annotations

import os
import gc
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import json

import numpy as np
import pandas as pd
import torch
from tqdm import tqdm

try:
    import faiss
except ImportError:
    print("Warning: faiss not installed. Install with: pip install faiss-cpu")
    faiss = None

try:
    from transformers import AutoModel, AutoTokenizer
except ImportError:
    print("Warning: transformers not installed. Install with: pip install transformers")
    raise


# ============================================================================
# CONFIGURATION
# ============================================================================

# Dataset root directory (output from generate_dinov3_dataset.py)
DATASET_ROOT = Path("dinov3_nifty50_dataset")

# Metadata CSV path
METADATA_CSV = DATASET_ROOT / "metadata" / "dataset-TRAIN.csv"

# Output directories
TS_EMBED_DIR = DATASET_ROOT / "ts_embeddings"
TS_EMBED_INDIVIDUAL_DIR = TS_EMBED_DIR / "individual"  # Individual .npy files
TS_FAISS_INDEX_PATH = TS_EMBED_DIR / "faiss_index.bin"
TS_METADATA_PATH = TS_EMBED_DIR / "metadata.json"

# Model configuration
MODEL_NAME = "Qwen/Qwen3-Embedding-4B"
BATCH_SIZE = 8  # Adjust based on available memory
MAX_SEQ_LENGTH = 8192  # Qwen3-Embedding supports up to 32K

# Device configuration
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"


# ============================================================================
# DATA LOADING
# ============================================================================

def load_ts_vectors_as_text(
    metadata_csv: Path,
    dataset_root: Path
) -> Tuple[List[str], List[Dict], List[str]]:
    """Load time-series vectors from .npy files and convert to text strings.

    Args:
        metadata_csv: Path to metadata CSV
        dataset_root: Root directory of the dataset

    Returns:
        Tuple of (texts, metadata_list, vector_names)
    """
    if not metadata_csv.exists():
        raise FileNotFoundError(f"Metadata CSV not found: {metadata_csv}")

    df = pd.read_csv(metadata_csv)
    print(f"Loaded metadata: {len(df)} entries")

    texts = []
    metadata_list = []
    vector_names = []

    for idx, row in df.iterrows():
        # Check if vector was successfully generated
        if row.get('vector_status') != 'success':
            continue
        if pd.isna(row.get('vector_path')):
            continue

        # Load vector
        vector_path = dataset_root / row['vector_path']
        if not vector_path.exists():
            print(f"Warning: Vector file not found: {vector_path}")
            continue

        # Load vector and convert to text string (just like in generate_embeddings.py)
        vec = np.load(vector_path)
        text = str(vec)  # Convert numpy array directly to string
        texts.append(text)

        # Collect metadata
        metadata_dict = {
            'image_name': row.get('image_name'),
            'class_id': row.get('class_id'),
            'class_index': row.get('class_index'),
            'segment_start_time': str(row.get('segment_start_time')),
            'segment_end_time': str(row.get('segment_end_time')),
            'segment_start_price': float(row.get('segment_start_price', 0)),
            'segment_end_price': float(row.get('segment_end_price', 0)),
            'vector_path': str(row.get('vector_path'))
        }
        metadata_list.append(metadata_dict)

        # Extract vector name (stem of the file)
        vector_name = Path(vector_path).stem
        vector_names.append(vector_name)

    print(f"Loaded {len(texts)} vectors as text")
    return texts, metadata_list, vector_names


# ============================================================================
# EMBEDDING GENERATION
# ============================================================================

class Qwen3TSEmbedder:
    """Qwen3-Embedding-4B wrapper for time-series text embedding generation."""

    def __init__(self, model_name: str = MODEL_NAME, device: str = DEVICE):
        """Initialize the Qwen3 embedding model.

        Args:
            model_name: Hugging Face model identifier
            device: Device to run inference on ('cuda' or 'cpu')
        """
        print(f"Loading model: {model_name}")
        print(f"Device: {device}")

        self.device = device
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        self.model = AutoModel.from_pretrained(
            model_name,
            trust_remote_code=True,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32
        ).to(device)
        self.model.eval()

        print("Model loaded successfully!")

    def encode_batch(self, texts: List[str], batch_size: int = BATCH_SIZE) -> np.ndarray:
        """Encode a batch of texts into embeddings.

        Args:
            texts: List of text strings to encode
            batch_size: Batch size for processing

        Returns:
            numpy array of shape (len(texts), embedding_dim)
        """
        all_embeddings = []

        for i in tqdm(range(0, len(texts), batch_size), desc="Generating embeddings"):
            batch_texts = texts[i:i + batch_size]

            # Tokenize with truncation
            inputs = self.tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=MAX_SEQ_LENGTH,
                return_tensors="pt"
            ).to(self.device)

            # Generate embeddings
            with torch.no_grad():
                outputs = self.model(**inputs)
                # Use mean pooling over token embeddings
                embeddings = outputs.last_hidden_state.mean(dim=1)
                embeddings = embeddings.cpu().numpy()

            all_embeddings.append(embeddings)

            # Memory cleanup
            del inputs, outputs
            if self.device == "cuda":
                torch.cuda.empty_cache()

        return np.vstack(all_embeddings).astype(np.float32)


# ============================================================================
# MAIN PROCESSING PIPELINE
# ============================================================================

def generate_and_save_embeddings(
    texts: List[str],
    vector_names: List[str],
    output_dir: Path
) -> np.ndarray:
    """Generate Qwen3 embeddings for time-series text representations.

    Args:
        texts: List of text representations of time-series vectors
        vector_names: List of vector names (for saving)
        output_dir: Directory to save individual embeddings

    Returns:
        numpy array of embeddings (n_vectors, embedding_dim)
    """
    output_dir.mkdir(parents=True, exist_ok=True)

    # Initialize model
    embedder = Qwen3TSEmbedder()

    # Generate embeddings in batches
    print(f"\nGenerating embeddings (batch size: {BATCH_SIZE})...")
    embeddings = embedder.encode_batch(texts, batch_size=BATCH_SIZE)

    # Save individual embeddings
    print("\nSaving individual embeddings...")
    for i, (embedding, name) in enumerate(tqdm(zip(embeddings, vector_names), total=len(embeddings))):
        output_path = output_dir / f"{name}.npy"
        np.save(output_path, embedding)

    print(f"Embeddings shape: {embeddings.shape}")
    print(f"Embedding dimension: {embeddings.shape[1]}")

    # Cleanup
    del embedder
    if DEVICE == "cuda":
        torch.cuda.empty_cache()
    gc.collect()

    return embeddings


def build_faiss_index(
    embeddings: np.ndarray,
    output_path: Path
) -> None:
    """Build and save a FAISS index for similarity search.

    Args:
        embeddings: numpy array of embeddings (n_vectors, embedding_dim)
        output_path: Path to save the FAISS index
    """
    if faiss is None:
        print("Warning: FAISS not available, skipping index creation")
        return

    print("\nBuilding FAISS index...")
    d = embeddings.shape[1]

    # Use IndexFlatIP for inner product (cosine similarity with normalized vectors)
    index = faiss.IndexFlatIP(d)

    # Normalize embeddings for cosine similarity
    faiss.normalize_L2(embeddings)

    # Add embeddings to index
    index.add(embeddings)

    # Save index
    output_path.parent.mkdir(parents=True, exist_ok=True)
    faiss.write_index(index, str(output_path))
    print(f"FAISS index saved: {output_path}")
    print(f"Index size: {index.ntotal} vectors")


def save_metadata(
    metadata_list: List[Dict],
    vector_names: List[str],
    output_path: Path
) -> None:
    """Save metadata mapping for retrieval.

    Args:
        metadata_list: List of metadata dicts
        vector_names: List of vector names
        output_path: Path to save metadata JSON
    """
    # Create mapping from index to metadata
    index_to_metadata = {}
    for i, (name, metadata) in enumerate(zip(vector_names, metadata_list)):
        index_to_metadata[i] = {
            'vector_name': name,
            **metadata
        }

    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w') as f:
        json.dump(index_to_metadata, f, indent=2)

    print(f"Metadata saved: {output_path}")


# ============================================================================
# RETRIEVAL FUNCTIONS
# ============================================================================

def load_faiss_index_and_metadata(
    index_path: Path = TS_FAISS_INDEX_PATH,
    metadata_path: Path = TS_METADATA_PATH
) -> Tuple[object, Dict]:
    """Load FAISS index and metadata for retrieval.

    Args:
        index_path: Path to FAISS index file
        metadata_path: Path to metadata JSON file

    Returns:
        Tuple of (faiss_index, metadata_dict)
    """
    if faiss is None:
        raise ImportError("FAISS not available. Install with: pip install faiss-cpu")

    if not index_path.exists():
        raise FileNotFoundError(f"FAISS index not found: {index_path}")
    if not metadata_path.exists():
        raise FileNotFoundError(f"Metadata not found: {metadata_path}")

    # Load FAISS index
    index = faiss.read_index(str(index_path))

    # Load metadata
    with open(metadata_path, 'r') as f:
        metadata = json.load(f)

    print(f"Loaded FAISS index: {index.ntotal} vectors")
    print(f"Loaded metadata: {len(metadata)} entries")

    return index, metadata


def retrieve_similar_segments(
    query_vector: np.ndarray,
    index: object,
    metadata: Dict,
    embedder: Optional[Qwen3TSEmbedder] = None,
    top_k: int = 10
) -> List[Dict]:
    """Retrieve similar time-series segments given a query vector.

    Args:
        query_vector: Query time-series vector (512-dim numpy array)
        index: FAISS index
        metadata: Metadata dictionary
        embedder: Qwen3TSEmbedder instance (will create if None)
        top_k: Number of results to return

    Returns:
        List of result dicts with metadata and scores
    """
    # Convert query vector to text (same as training)
    query_text = str(query_vector)

    # Initialize embedder if not provided
    if embedder is None:
        embedder = Qwen3TSEmbedder()

    # Generate embedding for query
    query_embedding = embedder.encode_batch([query_text], batch_size=1)

    # Normalize for cosine similarity
    faiss.normalize_L2(query_embedding)

    # Search
    scores, indices = index.search(query_embedding, top_k)

    # Build results
    results = []
    for score, idx in zip(scores[0], indices[0]):
        result = {
            'score': float(score),
            'index': int(idx),
            **metadata[str(idx)]
        }
        results.append(result)

    return results


def retrieve_by_ticker_and_date(
    ticker: str,
    start_date: str,
    index: object,
    metadata: Dict,
    top_k: int = 10
) -> List[Dict]:
    """Retrieve segments for a specific ticker and date range.

    Args:
        ticker: Stock ticker symbol (e.g., 'RELIANCE.NS')
        start_date: Start date for filtering
        index: FAISS index
        metadata: Metadata dictionary
        top_k: Maximum number of results

    Returns:
        List of result dicts matching the criteria
    """
    results = []
    for idx_str, meta in metadata.items():
        if meta['class_id'] == ticker:
            if start_date in meta['segment_start_time']:
                results.append({
                    'index': int(idx_str),
                    **meta
                })
                if len(results) >= top_k:
                    break

    return results


# ============================================================================
# MAIN
# ============================================================================

def main():
    """Main entry point."""
    print("=" * 70)
    print("Time-Series Embedding Generator (Qwen3-Embedding-4B)")
    print("=" * 70)
    print(f"Dataset: {DATASET_ROOT}")
    print(f"Model: {MODEL_NAME}")
    print(f"Device: {DEVICE}")
    print(f"Batch size: {BATCH_SIZE}")
    print("=" * 70)

    # Step 1: Load time-series vectors as text
    print("\nStep 1: Loading time-series vectors as text...")
    texts, metadata_list, vector_names = load_ts_vectors_as_text(
        METADATA_CSV,
        DATASET_ROOT
    )

    if len(texts) == 0:
        print("No time-series vectors found! Exiting.")
        return

    # Step 2: Generate embeddings
    print("\nStep 2: Generating embeddings...")
    embeddings = generate_and_save_embeddings(
        texts,
        vector_names,
        TS_EMBED_INDIVIDUAL_DIR
    )

    # Step 3: Build FAISS index
    print("\nStep 3: Building FAISS index...")
    build_faiss_index(embeddings, TS_FAISS_INDEX_PATH)

    # Step 4: Save metadata
    print("\nStep 4: Saving metadata...")
    save_metadata(metadata_list, vector_names, TS_METADATA_PATH)

    # Summary
    print("\n" + "=" * 70)
    print("Embedding generation complete!")
    print("=" * 70)
    print(f"Total vectors processed: {len(texts)}")
    print(f"Embedding dimension: {embeddings.shape[1]}")
    print(f"\nOutput structure:")
    print(f"  {TS_EMBED_DIR}/")
    print(f"  ├── individual/")
    print(f"  │   ├── TICKER_NS_seg_0000.npy")
    print(f"  │   └── ...")
    print(f"  ├── faiss_index.bin")
    print(f"  └── metadata.json")
    print("\n" + "=" * 70)
    print("Usage Example:")
    print("=" * 70)
    print("from generate_ts_embeddings import (")
    print("    load_faiss_index_and_metadata,")
    print("    retrieve_similar_segments,")
    print("    retrieve_by_ticker_and_date")
    print(")")
    print("")
    print("# Load index and metadata")
    print("index, metadata = load_faiss_index_and_metadata()")
    print("")
    print("# Retrieve by similar vector")
    print("query_vec = np.load('path/to/query_vector.npy')")
    print("results = retrieve_similar_segments(query_vec, index, metadata, top_k=10)")
    print("")
    print("# Retrieve by ticker and date")
    print("results = retrieve_by_ticker_and_date('RELIANCE.NS', '2025', index, metadata)")


if __name__ == "__main__":
    main()
